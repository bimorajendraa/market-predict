# Finance Analytics

End-to-end Python platform for financial data collection, parsing, scoring, sentiment analysis, and automated summary generation.

> Evolved from **finance-scraper** â€” original scraping pipeline is fully preserved.

## Features

- ðŸ“° **RSS News Scraping** â€” Collect news from multiple RSS feeds
- ðŸ“„ **Company Reports** â€” Crawl investor relations pages, download PDF/HTML reports
- ðŸ“Š **Financial Parsing** â€” Extract metrics from HTML/PDF reports with bilingual mapping (EN/ID)
- ðŸ† **Financial Scoring** â€” Score 0-100 with configurable weights (revenue growth, margins, FCF, D/E)
- ðŸ§  **News Sentiment** â€” FinBERT (English) + keyword dictionary (Indonesian) + event tagging
- ï¿½ **Market Prices** â€” Daily OHLCV from Yahoo Finance
- ï¿½ **Summary Generator** â€” Narrative from top financial drivers, news events, and returns
- ðŸª£ **MinIO Storage** â€” Raw file storage with structured keys
- ðŸ˜ **PostgreSQL** â€” 7 tables for full pipeline data
- ðŸ”„ **Prefect Orchestration** â€” Pipeline: scrape â†’ parse â†’ analyze â†’ summarize

## Tech Stack

- Python 3.11+, Poetry
- Docker Compose (Postgres 16, MinIO, Prefect Server)
- Libraries: requests, beautifulsoup4, lxml, feedparser, boto3, psycopg, prefect, playwright, pdfplumber, transformers (FinBERT), torch, yfinance

## Project Structure

```
finance-analytics/
â”œâ”€â”€ docker-compose.yml          # Docker services
â”œâ”€â”€ schema.sql                  # Database schema (7 tables)
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ inputs.example.json         # Example input config
â”œâ”€â”€ README.md
â””â”€â”€ app/
    â”œâ”€â”€ pyproject.toml           # Poetry dependencies
    â””â”€â”€ src/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ config.py            # Config + scoring weights
        â”œâ”€â”€ db.py                # PostgreSQL helpers
        â”œâ”€â”€ storage.py           # MinIO upload functions
        â”œâ”€â”€ main.py              # CLI entry point (8 commands)
        â”œâ”€â”€ collectors/
        â”‚   â”œâ”€â”€ base.py          # Base collector + retry logic
        â”‚   â”œâ”€â”€ news_rss.py      # RSS feed scraper
        â”‚   â””â”€â”€ company_reports.py
        â”œâ”€â”€ parsers/
        â”‚   â”œâ”€â”€ html_parser.py   # HTML report parser
        â”‚   â”œâ”€â”€ pdf_parser.py    # PDF report parser
        â”‚   â””â”€â”€ metric_mapper.py # Accountâ†’metric mapping + unit normalization
        â”œâ”€â”€ analysis/
        â”‚   â”œâ”€â”€ financial_scoring.py  # Scoring 0-100 + drivers
        â”‚   â””â”€â”€ news_sentiment.py     # FinBERT + event tagging
        â”œâ”€â”€ market/
        â”‚   â””â”€â”€ price_fetcher.py      # Yahoo Finance OHLCV
        â”œâ”€â”€ summary/
        â”‚   â””â”€â”€ generator.py          # Narrative summary generator
        â””â”€â”€ pipelines/
            â””â”€â”€ prefect_flow.py       # Orchestration flow
```

## Quick Start

### Step 1 â€” Start Docker Services

```bash
docker compose up -d
docker compose ps
```

| Service | URL | Credentials |
|---------|-----|-------------|
| PostgreSQL | localhost:5433 | ag / agpass |
| MinIO API | localhost:9000 | minio / minio12345 |
| MinIO Console | localhost:9001 | minio / minio12345 |
| Prefect UI | localhost:4200 | â€” |

### Step 2 â€” Install Python Dependencies

```bash
cd app

# Install Poetry if needed
pip install poetry

# Install all dependencies
poetry install

# Install Playwright browsers (for JS-rendered pages)
poetry run playwright install chromium
```

### Step 3 â€” Verify Database

```bash
docker exec -it ag-postgres psql -U ag -d antigravity -c "\dt"
```

Expected tables: `fetch_jobs`, `news_items`, `financial_facts`, `scores_financial`, `news_sentiment`, `market_prices`, `company_summary`

> **Note**: If you added tables after initial setup, re-apply the schema:
> ```bash
> docker exec -i ag-postgres psql -U ag -d antigravity < schema.sql
> ```

### Step 4 â€” Run the Pipeline (Step-by-Step)

From the `app/` directory:

```bash
# â”€â”€â”€ STEP A: Scrape â”€â”€â”€
# Collect news from RSS feeds
poetry run python -m src.main run-news
poetry run python -m src.main run-news -f "https://feeds.finance.yahoo.com/rss/2.0/headline?s=AAPL"

# Collect company reports (PDF/HTML)
poetry run python -m src.main run-reports
poetry run python -m src.main run-reports --playwright --limit 5

# â”€â”€â”€ STEP B: Parse â”€â”€â”€
# Parse downloaded reports â†’ financial_facts
poetry run python -m src.main run-parse --ticker AAPL

# â”€â”€â”€ STEP C: Fetch Market Data â”€â”€â”€
# Download OHLCV from Yahoo Finance
poetry run python -m src.main run-market --ticker AAPL --days 90

# â”€â”€â”€ STEP D: Analyze â”€â”€â”€
# Run financial scoring + news sentiment
poetry run python -m src.main run-analyze --ticker AAPL --period Q3-2025

# â”€â”€â”€ STEP E: Summarize â”€â”€â”€
# Generate narrative summary
poetry run python -m src.main run-summary --ticker AAPL --period Q3-2025
```

### Step 5 â€” Run Full Pipeline (Prefect Flow)

```bash
# Run all steps in sequence: scrape â†’ parse â†’ analyze â†’ summarize
poetry run python -m src.main run-flow --type all
```

## CLI Commands

| Command | Description | Key Options |
|---------|-------------|-------------|
| `run-news` | Scrape RSS feeds | `-f URL`, `-F file.json` |
| `run-reports` | Crawl & download reports | `-p URL`, `--playwright`, `--limit N` |
| `run-parse` | Parse reports â†’ financial_facts | `--ticker TICKER` |
| `run-market` | Fetch OHLCV from Yahoo Finance | `--ticker TICKER`, `--days N` |
| `run-analyze` | Financial scoring + sentiment | `--ticker TICKER`, `--period PERIOD` |
| `run-summary` | Generate narrative summary | `--ticker TICKER`, `--period PERIOD` |
| `run-flow` | Run Prefect orchestration | `--type all\|news\|reports` |
| `check-config` | Display current configuration | â€” |
| `init-storage` | Initialize MinIO bucket | â€” |

## Database Tables

### Original Tables

| Table | Purpose |
|-------|---------|
| `fetch_jobs` | Tracks all fetch operations (status, checksum, MinIO key) |
| `news_items` | Parsed news articles (title, body, URL dedup) |
| `financial_facts` | Extracted financial metrics (ticker, period, metric, value, unit, currency) |

### New Analytics Tables

| Table | Purpose |
|-------|---------|
| `scores_financial` | Financial scores 0-100 with `drivers_json` (explainable) |
| `news_sentiment` | Sentiment + impact + `events_json` + `sources_json` |
| `market_prices` | Daily OHLCV data (unique per ticker+date) |
| `company_summary` | Rating + narrative + `evidence_json` (explainable) |

## Financial Scoring

Metrics computed from `financial_facts`:

| Metric | Formula |
|--------|---------|
| `revenue_yoy` | (rev_current âˆ’ rev_prior_year) / rev_prior_year |
| `revenue_qoq` | (rev_current âˆ’ rev_prior_quarter) / rev_prior_quarter |
| `net_margin` | net_income / revenue |
| `op_margin` | operating_income / revenue |
| `ocf` | operating_cash_flow (normalized) |
| `fcf` | ocf âˆ’ capex |
| `debt_to_equity` | total_debt / total_equity |

**Weights** (configurable via env vars):

| Weight | Default | Env Var |
|--------|---------|---------|
| Revenue Growth (YoY) | 0.20 | `WEIGHT_REVENUE_GROWTH` |
| Net Margin | 0.15 | `WEIGHT_NET_MARGIN` |
| Operating Margin | 0.15 | `WEIGHT_OP_MARGIN` |
| Free Cash Flow | 0.15 | `WEIGHT_FCF` |
| Debt/Equity | 0.10 | `WEIGHT_DEBT_EQUITY` |
| Operating Cash Flow | 0.10 | `WEIGHT_OCF` |
| Revenue Growth (QoQ) | 0.15 | `WEIGHT_REVENUE_QOQ` |

## Supported Metric Mapping

The parser maps financial account names (EN + ID) to 10 standard metrics:

| Standard Metric | English Names | Indonesian Names |
|----------------|---------------|-----------------|
| `revenue` | Total Revenue, Net Sales | Pendapatan, Penjualan |
| `gross_profit` | Gross Profit | Laba Kotor |
| `operating_income` | Operating Income | Laba Usaha, Laba Operasi |
| `net_income` | Net Income, Net Profit | Laba Bersih |
| `operating_cash_flow` | Cash from Operations | Arus Kas dari Aktivitas Operasi |
| `capex` | Capital Expenditures | Belanja Modal |
| `total_assets` | Total Assets | Total Aset |
| `total_liabilities` | Total Liabilities | Total Liabilitas |
| `total_equity` | Total Equity | Total Ekuitas |
| `total_debt` | Total Debt | Total Utang |

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `POSTGRES_HOST` | localhost | PostgreSQL host |
| `POSTGRES_PORT` | 5432 | PostgreSQL port |
| `POSTGRES_USER` | ag | Database user |
| `POSTGRES_PASSWORD` | agpass | Database password |
| `POSTGRES_DB` | antigravity | Database name |
| `MINIO_ENDPOINT` | http://localhost:9000 | MinIO API |
| `MINIO_ACCESS_KEY` | minio | MinIO access key |
| `MINIO_SECRET_KEY` | minio12345 | MinIO secret key |
| `MINIO_BUCKET` | raw | Target bucket |
| `RATE_LIMIT_MIN` | 1 | Min delay (seconds) |
| `RATE_LIMIT_MAX` | 5 | Max delay (seconds) |
| `MAX_RETRIES` | 3 | Retry attempts |
| `LOG_LEVEL` | INFO | Logging level |
| `WEIGHT_*` | (see above) | Scoring weight overrides |

## Verify Results

```bash
# Check all tables
docker exec -it ag-postgres psql -U ag -d antigravity -c \
  "SELECT 'fetch_jobs' as t, count(*) FROM fetch_jobs
   UNION ALL SELECT 'news_items', count(*) FROM news_items
   UNION ALL SELECT 'financial_facts', count(*) FROM financial_facts
   UNION ALL SELECT 'scores_financial', count(*) FROM scores_financial
   UNION ALL SELECT 'news_sentiment', count(*) FROM news_sentiment
   UNION ALL SELECT 'market_prices', count(*) FROM market_prices
   UNION ALL SELECT 'company_summary', count(*) FROM company_summary;"

# View financial scores with drivers
docker exec -it ag-postgres psql -U ag -d antigravity -c \
  "SELECT ticker, period, score, drivers_json FROM scores_financial LIMIT 3;"

# View generated summaries
docker exec -it ag-postgres psql -U ag -d antigravity -c \
  "SELECT ticker, period, rating, LEFT(narrative, 100) FROM company_summary LIMIT 3;"
```

## Cleanup

```bash
docker compose down        # Stop services
docker compose down -v     # Stop + delete all data
```

## Notes

- **Explainability**: All outputs include `drivers_json` or `evidence_json` â€” no black-box results
- **No fabricated URLs**: All `source_url` fields come from actual scraped data or user input
- **Rate Limiting**: 1-5 second random delay between requests
- **Deduplication**: SHA256 checksum for content, UNIQUE constraint for market prices
- **FinBERT**: First run downloads ~420MB model weights. Subsequent runs use cache
- **Indonesian Support**: Financial keyword dictionary for ID-language reports and sentiment
